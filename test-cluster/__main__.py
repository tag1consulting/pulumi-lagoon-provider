"""
Lagoon Test Cluster Setup with Pulumi

This Pulumi program creates a local kind cluster and deploys Lagoon for testing
the pulumi-lagoon-provider.

Prerequisites:
- Docker installed and running
- kind CLI installed
- kubectl installed
- Pulumi installed

Usage:
    cd test-cluster
    pulumi stack init dev
    pulumi up
"""

import os
import pulumi
import pulumi_command as command
import pulumi_kubernetes as k8s
from pulumi_kubernetes.helm.v3 import Chart, ChartOpts, FetchOpts
import yaml

# Configuration
config = pulumi.Config()
cluster_name = config.get("clusterName") or "lagoon-test"
lagoon_values_path = os.path.join(os.path.dirname(__file__), "config", "lagoon-values.yaml")

# Ingress port configuration (allows overriding default ports 80/443)
# This is useful when these ports are already in use by other services
ingress_http_port = config.get_int("ingressHttpPort") or 80
ingress_https_port = config.get_int("ingressHttpsPort") or 443

# Generate Kind cluster configuration dynamically to support configurable ports
kind_config = f"""# Kind cluster configuration for Lagoon test environment
# This creates a three-node cluster (1 control-plane + 2 workers) for more realistic testing
# Generated by Pulumi with configurable ingress ports

kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
name: {cluster_name}

# Multi-node cluster for realistic testing
# Use stable K8s 1.29 image - newer images may have compatibility issues with Kind v0.31.0
nodes:
  # Control plane node
  - role: control-plane
    image: kindest/node:v1.29.0
    # Port mappings to access services from host
    extraPortMappings:
      # HTTP (configurable, default: 80)
      - containerPort: 80
        hostPort: {ingress_http_port}
        protocol: TCP
      # HTTPS (configurable, default: 443)
      - containerPort: 443
        hostPort: {ingress_https_port}
        protocol: TCP
      # Lagoon API (optional direct access)
      - containerPort: 3000
        hostPort: 3000
        protocol: TCP
      # NodePort mappings for Lagoon services
      - containerPort: 30030
        hostPort: 30030
        protocol: TCP
      - containerPort: 30370
        hostPort: 30370
        protocol: TCP
      - containerPort: 31311
        hostPort: 31311
        protocol: TCP
    # Mount for docker socket (if needed for builds)
    extraMounts:
      - hostPath: /var/run/docker.sock
        containerPath: /var/run/docker.sock
    # Labels for ingress controller
    kubeadmConfigPatches:
      - |
        kind: InitConfiguration
        nodeRegistration:
          kubeletExtraArgs:
            node-labels: "ingress-ready=true"

  # Worker node 1
  - role: worker
    image: kindest/node:v1.29.0
    extraMounts:
      - hostPath: /var/run/docker.sock
        containerPath: /var/run/docker.sock

  # Worker node 2
  - role: worker
    image: kindest/node:v1.29.0
    extraMounts:
      - hostPath: /var/run/docker.sock
        containerPath: /var/run/docker.sock

# Networking configuration
networking:
  # Use default pod/service subnets
  podSubnet: "10.244.0.0/16"
  serviceSubnet: "10.96.0.0/12"
  # Disable default CNI to use kind's default (kindnet)
  disableDefaultCNI: false

# Feature gates (if needed)
featureGates: {{}}

# Runtime configuration
runtimeConfig: {{}}
"""

# Write the Kind config to a temporary file
kind_config_path = os.path.join(os.path.dirname(__file__), "config", "kind-config-generated.yaml")
with open(kind_config_path, "w") as f:
    f.write(kind_config)

# Step 1: Create kind cluster
pulumi.log.info("Creating kind cluster...")

# Create kind cluster - the --wait flag ensures cluster is ready before returning
create_cluster = command.local.Command(
    "create-kind-cluster",
    create=f"kind create cluster --name {cluster_name} --config {kind_config_path} --wait 5m || kind get kubeconfig --name {cluster_name} > /dev/null 2>&1",
    delete=f"kind delete cluster --name {cluster_name}",
)

# Get kubeconfig for the cluster
get_kubeconfig = command.local.Command(
    "get-kubeconfig",
    create=f"kind get kubeconfig --name {cluster_name}",
    opts=pulumi.ResourceOptions(depends_on=[create_cluster]),
)

# Create Kubernetes provider using the kind cluster
k8s_provider = k8s.Provider(
    "kind-k8s-provider",
    kubeconfig=get_kubeconfig.stdout,
    opts=pulumi.ResourceOptions(depends_on=[get_kubeconfig]),
)

# Step 2: Install ingress-nginx
pulumi.log.info("Installing ingress-nginx...")

# Create ingress-nginx namespace first
ingress_nginx_namespace = k8s.core.v1.Namespace(
    "ingress-nginx",
    metadata={"name": "ingress-nginx"},
    opts=pulumi.ResourceOptions(provider=k8s_provider),
)

ingress_nginx = Chart(
    "ingress-nginx",
    ChartOpts(
        chart="ingress-nginx",
        version="4.11.3",
        namespace="ingress-nginx",
        fetch_opts=FetchOpts(
            repo="https://kubernetes.github.io/ingress-nginx",
        ),
        values={
            "controller": {
                "hostNetwork": True,
                "service": {
                    "type": "NodePort",
                },
                "ingressClassResource": {
                    "default": True,
                },
                "watchIngressWithoutClass": True,
                "nodeSelector": {
                    "ingress-ready": "true",
                },
                "tolerations": [
                    {
                        "key": "node-role.kubernetes.io/control-plane",
                        "operator": "Equal",
                        "effect": "NoSchedule",
                    },
                    {
                        "key": "node-role.kubernetes.io/master",
                        "operator": "Equal",
                        "effect": "NoSchedule",
                    },
                ],
            },
        },
    ),
    opts=pulumi.ResourceOptions(
        provider=k8s_provider,
        depends_on=[ingress_nginx_namespace],
    ),
)

# Step 3: Install cert-manager
pulumi.log.info("Installing cert-manager...")

# Create cert-manager namespace first
cert_manager_namespace = k8s.core.v1.Namespace(
    "cert-manager",
    metadata={"name": "cert-manager"},
    opts=pulumi.ResourceOptions(provider=k8s_provider),
)

cert_manager = Chart(
    "cert-manager",
    ChartOpts(
        chart="cert-manager",
        version="v1.16.2",
        namespace="cert-manager",
        fetch_opts=FetchOpts(
            repo="https://charts.jetstack.io",
        ),
        values={
            "installCRDs": True,  # Let Helm install CRDs
            "global": {
                "leaderElection": {
                    "namespace": "cert-manager",
                },
            },
        },
    ),
    opts=pulumi.ResourceOptions(
        provider=k8s_provider,
        depends_on=[cert_manager_namespace, ingress_nginx],  # Wait for namespace and ingress
    ),
)

# Create self-signed issuer for local testing
# NOTE: Disabled because ClusterIssuer CRDs from cert-manager are not ready on first deployment
# This can be created manually after deployment if needed:
# kubectl apply -f - <<EOF
# apiVersion: cert-manager.io/v1
# kind: ClusterIssuer
# metadata:
#   name: selfsigned
# spec:
#   selfSigned: {}
# EOF
#
# selfsigned_issuer = k8s.apiextensions.CustomResource(
#     "selfsigned-issuer",
#     api_version="cert-manager.io/v1",
#     kind="ClusterIssuer",
#     metadata={
#         "name": "selfsigned",
#     },
#     spec={
#         "selfSigned": {},
#     },
#     opts=pulumi.ResourceOptions(
#         provider=k8s_provider,
#         depends_on=[cert_manager],
#     ),
# )

# Step 3.5: Install metrics-server
pulumi.log.info("Installing metrics-server...")

metrics_server = Chart(
    "metrics-server",
    ChartOpts(
        chart="metrics-server",
        version="3.12.2",
        namespace="kube-system",
        fetch_opts=FetchOpts(
            repo="https://kubernetes-sigs.github.io/metrics-server/",
        ),
        values={
            "args": [
                "--kubelet-insecure-tls",  # Required for kind
                "--kubelet-preferred-address-types=InternalIP",
            ],
        },
    ),
    opts=pulumi.ResourceOptions(
        provider=k8s_provider,
        depends_on=[ingress_nginx],
    ),
)

# Wait for ingress-nginx webhook to be ready
ingress_webhook_wait_script = f"""
echo "Waiting for ingress-nginx webhook to be ready..."
# Wait for controller deployment to exist
while ! kubectl --context kind-{cluster_name} get deployment -n ingress-nginx ingress-nginx-controller 2>/dev/null; do
    echo "Waiting for ingress controller deployment to be created..."
    sleep 2
done
# Wait for controller pods to be ready
kubectl --context kind-{cluster_name} wait --for=condition=ready pod \
    -l app.kubernetes.io/component=controller \
    -n ingress-nginx --timeout=300s || {{
    echo "Warning: Ingress controller not ready. May be expected on updates."
    exit 0
}}
echo "Ingress controller pods ready, waiting for webhook endpoint..."
# Wait for admission service to have endpoints
for i in {{1..30}}; do
    ENDPOINTS=$(kubectl --context kind-{cluster_name} get endpoints -n ingress-nginx ingress-nginx-controller-admission -o jsonpath='{{.subsets[*].addresses[*].ip}}' 2>/dev/null)
    if [ -n "$ENDPOINTS" ]; then
        echo "Webhook endpoints ready: $ENDPOINTS"
        break
    fi
    echo "Waiting for webhook endpoints (attempt $i/30)..."
    sleep 2
done
echo "Adding extra delay for webhook to be fully operational..."
sleep 10
echo "Ingress webhook should be ready now."
"""

wait_for_ingress_webhook = command.local.Command(
    "wait-for-ingress-webhook",
    create=ingress_webhook_wait_script,
    update=ingress_webhook_wait_script,
    opts=pulumi.ResourceOptions(depends_on=[ingress_nginx, get_kubeconfig]),
)

# Step 3.75: Generate TLS certificates for *.lagoon.test
pulumi.log.info("Generating TLS certificates...")

# Create directory for certificates
cert_dir = "/tmp/lagoon-certs"
create_cert_dir = command.local.Command(
    "create-cert-dir",
    create=f"mkdir -p {cert_dir}",
    opts=pulumi.ResourceOptions(depends_on=[get_kubeconfig]),
)

# Generate CA certificate
generate_ca_script = f"""
cd {cert_dir}
# Only generate if it doesn't exist
if [ ! -f ca.crt ]; then
    echo "Generating CA certificate..."
    openssl genrsa -out ca.key 2048
    openssl req -new -x509 -days 365 -key ca.key -out ca.crt \
      -subj "/CN=Lagoon Test CA"
    echo "CA certificate generated"
else
    echo "CA certificate already exists"
fi
"""

generate_ca = command.local.Command(
    "generate-ca",
    create=generate_ca_script,
    opts=pulumi.ResourceOptions(depends_on=[create_cert_dir]),
)

# Generate wildcard certificate for *.lagoon.test
generate_cert_script = f"""
cd {cert_dir}
# Only generate if it doesn't exist
if [ ! -f lagoon-test.crt ]; then
    echo "Generating wildcard certificate for *.lagoon.test..."

    # Generate private key
    openssl genrsa -out lagoon-test.key 2048

    # Generate CSR
    openssl req -new -key lagoon-test.key -out lagoon-test.csr \
      -subj "/CN=*.lagoon.test"

    # Create extension file with SAN
    cat > lagoon-test.ext << EXTEOF
authorityKeyIdentifier=keyid,issuer
basicConstraints=CA:FALSE
keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment
subjectAltName = @alt_names

[alt_names]
DNS.1 = *.lagoon.test
DNS.2 = lagoon.test
DNS.3 = ui.lagoon.test
DNS.4 = api.lagoon.test
DNS.5 = keycloak.lagoon.test
DNS.6 = harbor.lagoon.test
EXTEOF

    # Sign the certificate
    openssl x509 -req -in lagoon-test.csr -CA ca.crt -CAkey ca.key \
      -CAcreateserial -out lagoon-test.crt -days 365 \
      -extfile lagoon-test.ext

    echo "Wildcard certificate generated"
else
    echo "Wildcard certificate already exists"
fi
"""

generate_cert = command.local.Command(
    "generate-cert",
    create=generate_cert_script,
    opts=pulumi.ResourceOptions(depends_on=[generate_ca]),
)

# Step 4: Install Harbor registry
pulumi.log.info("Installing Harbor registry...")

# Create harbor namespace
harbor_namespace = k8s.core.v1.Namespace(
    "harbor",
    metadata={"name": "harbor"},
    opts=pulumi.ResourceOptions(provider=k8s_provider),
)

# Create TLS secret for harbor namespace (before Harbor deployment)
# Read certificate and key files
def read_cert_files(_):
    with open(f"{cert_dir}/lagoon-test.crt", "r") as f:
        cert_content = f.read()
    with open(f"{cert_dir}/lagoon-test.key", "r") as f:
        key_content = f.read()
    return {"cert": cert_content, "key": key_content}

cert_data = generate_cert.stdout.apply(read_cert_files)

# TLS secret for harbor namespace
harbor_tls_secret = k8s.core.v1.Secret(
    "harbor-tls-secret",
    metadata={
        "name": "lagoon-test-tls",
        "namespace": "harbor",
    },
    type="kubernetes.io/tls",
    string_data={
        "tls.crt": cert_data["cert"],
        "tls.key": cert_data["key"],
    },
    opts=pulumi.ResourceOptions(
        provider=k8s_provider,
        depends_on=[harbor_namespace, generate_cert],
    ),
)

harbor = Chart(
    "harbor",
    ChartOpts(
        chart="harbor",
        version="1.16.1",
        namespace="harbor",
        fetch_opts=FetchOpts(
            repo="https://helm.goharbor.io",
        ),
        values={
            "expose": {
                "type": "clusterIP",  # Disable Harbor's built-in ingress, we'll create it separately
                "tls": {
                    "enabled": False,
                },
                "clusterIP": {
                    "name": "harbor",
                    "ports": {
                        "httpPort": 80,
                    },
                },
            },
            "externalURL": "https://harbor.lagoon.test",
            "persistence": {
                "enabled": True,
                "persistentVolumeClaim": {
                    "registry": {
                        "size": "5Gi",
                    },
                    "database": {
                        "size": "1Gi",
                    },
                },
            },
            # Use simple passwords for local testing
            "harborAdminPassword": "Harbor12345",
            "database": {
                "type": "internal",
            },
            "redis": {
                "type": "internal",
            },
        },
    ),
    opts=pulumi.ResourceOptions(
        provider=k8s_provider,
        depends_on=[harbor_namespace, cert_manager],  # No longer depends on wait_for_ingress_webhook
    ),
)

# Create Harbor ingress separately after webhook is ready
harbor_ingress = k8s.networking.v1.Ingress(
    "harbor-ingress",
    metadata={
        "name": "harbor",
        "namespace": "harbor",
    },
    spec={
        "ingressClassName": "nginx",
        "tls": [
            {
                "hosts": ["harbor.lagoon.test"],
                "secretName": "lagoon-test-tls",
            },
        ],
        "rules": [
            {
                "host": "harbor.lagoon.test",
                "http": {
                    "paths": [
                        {
                            "path": "/",
                            "pathType": "Prefix",
                            "backend": {
                                "service": {
                                    "name": "harbor",
                                    "port": {"number": 80},
                                },
                            },
                        },
                    ],
                },
            },
        ],
    },
    opts=pulumi.ResourceOptions(
        provider=k8s_provider,
        depends_on=[harbor, wait_for_ingress_webhook, harbor_tls_secret],
    ),
)

# Step 5: Install Lagoon
pulumi.log.info("Installing Lagoon...")

# Create lagoon namespace
lagoon_namespace = k8s.core.v1.Namespace(
    "lagoon",
    metadata={"name": "lagoon"},
    opts=pulumi.ResourceOptions(provider=k8s_provider),
)

# TLS secret for lagoon namespace
lagoon_tls_secret = k8s.core.v1.Secret(
    "lagoon-tls-secret",
    metadata={
        "name": "lagoon-test-tls",
        "namespace": "lagoon",
    },
    type="kubernetes.io/tls",
    string_data={
        "tls.crt": cert_data["cert"],
        "tls.key": cert_data["key"],
    },
    opts=pulumi.ResourceOptions(
        provider=k8s_provider,
        depends_on=[lagoon_namespace, generate_cert],
    ),
)

# Read lagoon values from file
with open(lagoon_values_path, "r") as f:
    lagoon_values = yaml.safe_load(f)

# Install lagoon-core
lagoon_core = Chart(
    "lagoon-core",
    ChartOpts(
        chart="lagoon-core",
        version="1.55.0",
        namespace="lagoon",
        fetch_opts=FetchOpts(
            repo="https://uselagoon.github.io/lagoon-charts/",
        ),
        values=lagoon_values,
    ),
    opts=pulumi.ResourceOptions(
        provider=k8s_provider,
        depends_on=[lagoon_namespace, harbor, cert_manager],
    ),
)

# Wait for broker secret and extract RabbitMQ password
# This command waits for the broker pod to be ready, then outputs the password
# The password is output to stdout so Pulumi can capture it as an Output
get_rabbitmq_password_script = f"""
set -e

# First wait for the broker StatefulSet pod to exist (kubectl wait fails if no resources match)
# Use statefulset.kubernetes.io/pod-name label to exclude the bootstrap job pod
echo "Waiting for broker pod to exist..." >&2
for i in $(seq 1 60); do
    if kubectl --context kind-{cluster_name} get pod lagoon-core-broker-0 -n lagoon 2>/dev/null | grep -q Running; then
        echo "Broker pod found and running" >&2
        break
    fi
    if [ $i -eq 60 ]; then
        echo "Timeout waiting for broker pod to exist" >&2
        exit 1
    fi
    echo "Waiting for broker pod... (attempt $i/60)" >&2
    sleep 5
done

# Now wait for broker pod to be ready (using specific pod name)
echo "Waiting for broker pod to be ready..." >&2
kubectl --context kind-{cluster_name} wait --for=condition=ready pod/lagoon-core-broker-0 -n lagoon --timeout=300s >&2

# Wait for the secret to exist
echo "Waiting for broker secret..." >&2
for i in $(seq 1 30); do
    if kubectl --context kind-{cluster_name} get secret lagoon-core-broker -n lagoon 2>/dev/null | grep -q lagoon-core-broker; then
        echo "Broker secret found" >&2
        break
    fi
    if [ $i -eq 30 ]; then
        echo "Timeout waiting for broker secret" >&2
        exit 1
    fi
    sleep 2
done

# Get the password (base64 encoded as stored in the secret)
RABBIT_PASS=$(kubectl --context kind-{cluster_name} get secret lagoon-core-broker -n lagoon -o jsonpath='{{.data.RABBITMQ_PASSWORD}}')

# Output the base64-encoded password (stdout only - no other output)
echo -n "$RABBIT_PASS"
"""

get_rabbitmq_password_cmd = command.local.Command(
    "get-rabbitmq-password",
    create=get_rabbitmq_password_script,
    opts=pulumi.ResourceOptions(depends_on=[lagoon_core, get_kubeconfig]),
)

# Install lagoon-build-deploy (remote controller)
# Note: We don't set rabbitMQPassword in the chart values because
# we'll create the secret ourselves with the correct password
lagoon_build_deploy = Chart(
    "lagoon-build-deploy",
    ChartOpts(
        chart="lagoon-build-deploy",
        version="0.35.0",
        namespace="lagoon",
        fetch_opts=FetchOpts(
            repo="https://uselagoon.github.io/lagoon-charts/",
        ),
        values={
            "lagoonTargetName": "local-kind",
            "rabbitMQHostname": "lagoon-core-broker.lagoon.svc.cluster.local",
            "rabbitMQUsername": "lagoon",
            "rabbitMQPassword": "placeholder",  # Placeholder - secret will be patched below
            "registry": {
                "name": "harbor.lagoon.test",
                "username": "admin",
                "password": "Harbor12345",
            },
        },
    ),
    opts=pulumi.ResourceOptions(
        provider=k8s_provider,
        depends_on=[lagoon_core, harbor, get_rabbitmq_password_cmd],
    ),
)

# Patch the lagoon-build-deploy secret with the correct RabbitMQ password
# Using kubectl patch preserves other fields in the secret while updating only RABBITMQ_PASSWORD
# The password is already base64-encoded from the source secret
def create_patch_script(password_b64: str) -> str:
    return f"""
set -e

# Wait for lagoon-build-deploy secret to exist (Helm chart may not have finished creating it)
echo "Waiting for lagoon-build-deploy secret to exist..."
for i in $(seq 1 60); do
    if kubectl --context kind-{cluster_name} get secret lagoon-build-deploy -n lagoon 2>/dev/null | grep -q lagoon-build-deploy; then
        echo "Secret found"
        break
    fi
    if [ $i -eq 60 ]; then
        echo "Timeout waiting for lagoon-build-deploy secret"
        exit 1
    fi
    echo "Waiting for secret... (attempt $i/60)"
    sleep 5
done

# Patch the secret with the correct password
kubectl --context kind-{cluster_name} patch secret lagoon-build-deploy -n lagoon \
  --type='json' \
  -p='[{{"op":"replace","path":"/data/RABBITMQ_PASSWORD","value":"{password_b64}"}}]'
echo "Secret patched successfully"
"""

patch_build_deploy_secret = command.local.Command(
    "patch-build-deploy-secret",
    create=get_rabbitmq_password_cmd.stdout.apply(create_patch_script),
    # Run patch on every update to ensure password stays in sync
    update=get_rabbitmq_password_cmd.stdout.apply(create_patch_script),
    # Triggers ensure the patch runs when the password changes
    triggers=[get_rabbitmq_password_cmd.stdout],
    opts=pulumi.ResourceOptions(depends_on=[lagoon_build_deploy, get_kubeconfig]),
)

# Restart the lagoon-build-deploy deployment to pick up the new secret
# This ensures the pods use the correct RabbitMQ password
restart_build_deploy = command.local.Command(
    "restart-build-deploy",
    create=f"kubectl --context kind-{cluster_name} rollout restart deployment lagoon-build-deploy -n lagoon && kubectl --context kind-{cluster_name} rollout status deployment lagoon-build-deploy -n lagoon --timeout=120s",
    # Use triggers to ensure restart happens when secret changes
    triggers=[get_rabbitmq_password_cmd.stdout],
    opts=pulumi.ResourceOptions(depends_on=[patch_build_deploy_secret, get_kubeconfig]),
)

# Create lagoonadmin user in Keycloak
create_lagoon_admin_script = f"""
set -e

KEYCLOAK_URL="https://keycloak.lagoon.test/auth"

# Wait for Keycloak to be ready
echo "Waiting for Keycloak to be ready..."
kubectl --context kind-{cluster_name} wait --for=condition=ready pod -l app.kubernetes.io/name=lagoon-core,app.kubernetes.io/component=lagoon-core-keycloak -n lagoon --timeout=300s || {{
    echo "Warning: Keycloak not ready yet. User creation may fail."
    exit 0
}}

# Get admin credentials
ADMIN_USER="admin"
ADMIN_PASS=$(kubectl --context kind-{cluster_name} get secret lagoon-core-keycloak -n lagoon -o jsonpath='{{.data.KEYCLOAK_ADMIN_PASSWORD}}' | base64 -d)
LAGOON_ADMIN_PASS=$(kubectl --context kind-{cluster_name} get secret lagoon-core-keycloak -n lagoon -o jsonpath='{{.data.KEYCLOAK_LAGOON_ADMIN_PASSWORD}}' | base64 -d)

if [ -z "$ADMIN_PASS" ] || [ -z "$LAGOON_ADMIN_PASS" ]; then
    echo "Warning: Could not get Keycloak credentials. Secrets may not exist yet."
    exit 0
fi

# Wait a bit more for Keycloak to be fully operational
sleep 10

echo "Getting Keycloak admin token..."
TOKEN_RESPONSE=$(curl -k -s -X POST "${{KEYCLOAK_URL}}/realms/master/protocol/openid-connect/token" \
  -H "Content-Type: application/x-www-form-urlencoded" \
  -d "username=${{ADMIN_USER}}" \
  -d "password=${{ADMIN_PASS}}" \
  -d "grant_type=password" \
  -d "client_id=admin-cli")

ADMIN_TOKEN=$(echo "$TOKEN_RESPONSE" | grep -o '"access_token":"[^"]*"' | cut -d'"' -f4)

if [ -z "$ADMIN_TOKEN" ]; then
    echo "Warning: Failed to get admin token. Keycloak may not be fully ready yet."
    exit 0
fi

echo "Creating lagoonadmin user..."
CREATE_RESPONSE=$(curl -k -s -w "\\nHTTP_CODE:%{{http_code}}" -X POST "${{KEYCLOAK_URL}}/admin/realms/lagoon/users" \
  -H "Authorization: Bearer ${{ADMIN_TOKEN}}" \
  -H "Content-Type: application/json" \
  -d '{{
    "username": "lagoonadmin",
    "email": "admin@lagoon.local",
    "firstName": "Lagoon",
    "lastName": "Admin",
    "enabled": true,
    "emailVerified": true,
    "credentials": [{{
      "type": "password",
      "value": "'"${{LAGOON_ADMIN_PASS}}"'",
      "temporary": false
    }}]
  }}')

HTTP_CODE=$(echo "$CREATE_RESPONSE" | grep "HTTP_CODE:" | cut -d: -f2)

if [ "$HTTP_CODE" = "201" ]; then
  echo "✓ lagoonadmin user created successfully"
elif [ "$HTTP_CODE" = "409" ]; then
  echo "✓ lagoonadmin user already exists"
else
  echo "Warning: Unexpected response (HTTP $HTTP_CODE) when creating user"
  exit 0
fi

# Get user ID and assign admin role
USER_RESPONSE=$(curl -k -s -X GET "${{KEYCLOAK_URL}}/admin/realms/lagoon/users?username=lagoonadmin&exact=true" \
  -H "Authorization: Bearer ${{ADMIN_TOKEN}}")

USER_ID=$(echo "$USER_RESPONSE" | grep -o '"id":"[^"]*"' | head -1 | cut -d'"' -f4)

if [ -n "$USER_ID" ]; then
  ROLE_RESPONSE=$(curl -k -s -X GET "${{KEYCLOAK_URL}}/admin/realms/lagoon/roles/admin" \
    -H "Authorization: Bearer ${{ADMIN_TOKEN}}")

  ROLE_ID=$(echo "$ROLE_RESPONSE" | grep -o '"id":"[^"]*"' | head -1 | cut -d'"' -f4)

  if [ -n "$ROLE_ID" ]; then
    curl -k -s -X POST "${{KEYCLOAK_URL}}/admin/realms/lagoon/users/${{USER_ID}}/role-mappings/realm" \
      -H "Authorization: Bearer ${{ADMIN_TOKEN}}" \
      -H "Content-Type: application/json" \
      -d '[{{
        "id": "'"${{ROLE_ID}}"'",
        "name": "admin"
      }}]'
    echo "✓ Admin role assigned to lagoonadmin user"
  fi
fi

echo "Lagoon admin user setup complete"
"""

create_lagoon_admin = command.local.Command(
    "create-lagoon-admin",
    create=create_lagoon_admin_script,
    update=create_lagoon_admin_script,
    opts=pulumi.ResourceOptions(depends_on=[lagoon_core, get_kubeconfig]),
)

# Create Ingress resources for domain-based access
lagoon_api_ingress = k8s.networking.v1.Ingress(
    "lagoon-api-ingress",
    metadata={
        "name": "lagoon-api",
        "namespace": "lagoon",
        "annotations": {
            "nginx.ingress.kubernetes.io/rewrite-target": "/",
        },
    },
    spec={
        "ingressClassName": "nginx",
        "tls": [
            {
                "hosts": ["api.lagoon.test"],
                "secretName": "lagoon-test-tls",
            },
        ],
        "rules": [
            {
                "host": "api.lagoon.test",
                "http": {
                    "paths": [
                        {
                            "path": "/",
                            "pathType": "Prefix",
                            "backend": {
                                "service": {
                                    "name": "lagoon-core-api",
                                    "port": {"number": 80},
                                },
                            },
                        },
                    ],
                },
            },
        ],
    },
    opts=pulumi.ResourceOptions(
        provider=k8s_provider,
        depends_on=[lagoon_core, wait_for_ingress_webhook, lagoon_tls_secret],
    ),
)

lagoon_keycloak_ingress = k8s.networking.v1.Ingress(
    "lagoon-keycloak-ingress",
    metadata={
        "name": "lagoon-keycloak",
        "namespace": "lagoon",
    },
    spec={
        "ingressClassName": "nginx",
        "tls": [
            {
                "hosts": ["keycloak.lagoon.test"],
                "secretName": "lagoon-test-tls",
            },
        ],
        "rules": [
            {
                "host": "keycloak.lagoon.test",
                "http": {
                    "paths": [
                        {
                            "path": "/",
                            "pathType": "Prefix",
                            "backend": {
                                "service": {
                                    "name": "lagoon-core-keycloak",
                                    "port": {"number": 8080},
                                },
                            },
                        },
                    ],
                },
            },
        ],
    },
    opts=pulumi.ResourceOptions(
        provider=k8s_provider,
        depends_on=[lagoon_core, wait_for_ingress_webhook, lagoon_tls_secret],
    ),
)

lagoon_ui_ingress = k8s.networking.v1.Ingress(
    "lagoon-ui-ingress",
    metadata={
        "name": "lagoon-ui",
        "namespace": "lagoon",
    },
    spec={
        "ingressClassName": "nginx",
        "tls": [
            {
                "hosts": ["ui.lagoon.test"],
                "secretName": "lagoon-test-tls",
            },
        ],
        "rules": [
            {
                "host": "ui.lagoon.test",
                "http": {
                    "paths": [
                        {
                            "path": "/",
                            "pathType": "Prefix",
                            "backend": {
                                "service": {
                                    "name": "lagoon-core-ui",
                                    "port": {"number": 3000},
                                },
                            },
                        },
                    ],
                },
            },
        ],
    },
    opts=pulumi.ResourceOptions(
        provider=k8s_provider,
        depends_on=[lagoon_core, wait_for_ingress_webhook, lagoon_tls_secret],
    ),
)

# Configure services as NodePort for easier access
api_nodeport_cmd = f"kubectl --context kind-{cluster_name} patch svc lagoon-core-api -n lagoon -p '{{\"spec\":{{\"type\":\"NodePort\",\"ports\":[{{\"port\":80,\"targetPort\":3000,\"nodePort\":30030}}]}}}}' || echo 'Service already configured'"

keycloak_nodeport_cmd = f"kubectl --context kind-{cluster_name} patch svc lagoon-core-keycloak -n lagoon -p '{{\"spec\":{{\"type\":\"NodePort\",\"ports\":[{{\"name\":\"http\",\"port\":8080,\"targetPort\":8080,\"nodePort\":30370}},{{\"name\":\"management\",\"port\":9000,\"targetPort\":9000,\"nodePort\":30371}}]}}}}' || echo 'Service already configured'"

ui_nodeport_cmd = f"kubectl --context kind-{cluster_name} patch svc lagoon-core-ui -n lagoon -p '{{\"spec\":{{\"type\":\"NodePort\",\"ports\":[{{\"port\":3000,\"targetPort\":3000,\"nodePort\":31311}}]}}}}' || echo 'Service already configured'"

patch_api_nodeport = command.local.Command(
    "patch-api-nodeport",
    create=api_nodeport_cmd,
    update=api_nodeport_cmd,
    opts=pulumi.ResourceOptions(depends_on=[lagoon_core, get_kubeconfig]),
)

patch_keycloak_nodeport = command.local.Command(
    "patch-keycloak-nodeport",
    create=keycloak_nodeport_cmd,
    update=keycloak_nodeport_cmd,
    opts=pulumi.ResourceOptions(depends_on=[lagoon_core, get_kubeconfig]),
)

patch_ui_nodeport = command.local.Command(
    "patch-ui-nodeport",
    create=ui_nodeport_cmd,
    update=ui_nodeport_cmd,
    opts=pulumi.ResourceOptions(depends_on=[lagoon_core, get_kubeconfig]),
)

# Export important information
pulumi.export("cluster_name", cluster_name)
pulumi.export("kubeconfig_command", f"kind get kubeconfig --name {cluster_name}")

# Export configured ingress ports
pulumi.export("ingress_http_port", ingress_http_port)
pulumi.export("ingress_https_port", ingress_https_port)

# Access URLs (via NodePort - kind maps these to localhost)
pulumi.export("lagoon_api_url", "http://localhost:30030/graphql")
pulumi.export("keycloak_url", pulumi.Output.all(patch_keycloak_nodeport.stdout).apply(
    lambda _: "http://localhost:30370/auth"
))
pulumi.export("lagoon_ui_url", pulumi.Output.all(patch_ui_nodeport.stdout).apply(
    lambda _: "http://localhost:31311"
))

# Commands to get NodePort assignments
pulumi.export(
    "get_nodeports",
    f"kubectl --context kind-{cluster_name} get svc -n lagoon | grep NodePort",
)

# Credentials
pulumi.export("harbor_username", "admin")
pulumi.export("harbor_password", "Harbor12345")

# Keycloak admin (master realm)
pulumi.export(
    "keycloak_admin_username",
    "admin"
)
pulumi.export(
    "keycloak_admin_password_cmd",
    f"kubectl --context kind-{cluster_name} get secret lagoon-core-keycloak -n lagoon -o jsonpath='{{.data.KEYCLOAK_ADMIN_PASSWORD}}' | base64 -d",
)

# Lagoon admin (lagoon realm) - for UI access
pulumi.export(
    "lagoon_admin_username",
    "lagoonadmin"
)
pulumi.export(
    "lagoon_admin_password_cmd",
    f"kubectl --context kind-{cluster_name} get secret lagoon-core-keycloak -n lagoon -o jsonpath='{{.data.KEYCLOAK_LAGOON_ADMIN_PASSWORD}}' | base64 -d",
)

pulumi.export(
    "get_rabbitmq_password",
    f"kubectl --context kind-{cluster_name} get secret lagoon-core-broker -n lagoon -o jsonpath='{{.data.RABBITMQ_PASSWORD}}' | base64 -d",
)

pulumi.log.info("=" * 80)
pulumi.log.info("Lagoon test cluster setup complete!")
pulumi.log.info("=" * 80)
pulumi.log.info("")
pulumi.log.info(f"Ingress ports: HTTP={ingress_http_port}, HTTPS={ingress_https_port}")
if ingress_http_port != 80 or ingress_https_port != 443:
    pulumi.log.info("  (Custom ports configured via ingressHttpPort/ingressHttpsPort)")
pulumi.log.info("")
pulumi.log.info("Domain-based URLs (via Ingress with HTTPS):")
pulumi.log.info("")
https_port_suffix = "" if ingress_https_port == 443 else f":{ingress_https_port}"
pulumi.log.info("1. Lagoon UI:")
pulumi.log.info(f"   https://ui.lagoon.test{https_port_suffix}")
pulumi.log.info("")
pulumi.log.info("2. Lagoon API (GraphQL):")
pulumi.log.info(f"   https://api.lagoon.test{https_port_suffix}/graphql")
pulumi.log.info("")
pulumi.log.info("3. Keycloak:")
pulumi.log.info(f"   https://keycloak.lagoon.test{https_port_suffix}/auth")
pulumi.log.info("")
pulumi.log.info("4. Harbor Registry:")
pulumi.log.info(f"   https://harbor.lagoon.test{https_port_suffix}")
pulumi.log.info("")
pulumi.log.info("Credentials:")
pulumi.log.info("")
pulumi.log.info("  Lagoon UI Login:")
pulumi.log.info("    Username: lagoonadmin")
pulumi.log.info("    Password: (run command below)")
pulumi.log.info(f"    $ kubectl --context kind-{cluster_name} get secret lagoon-core-keycloak -n lagoon -o jsonpath='{{.data.KEYCLOAK_LAGOON_ADMIN_PASSWORD}}' | base64 -d")
pulumi.log.info("")
pulumi.log.info("  Keycloak Admin (Master Realm):")
pulumi.log.info("    Username: admin")
pulumi.log.info("    Password: (run command below)")
pulumi.log.info(f"    $ kubectl --context kind-{cluster_name} get secret lagoon-core-keycloak -n lagoon -o jsonpath='{{.data.KEYCLOAK_ADMIN_PASSWORD}}' | base64 -d")
pulumi.log.info("")
pulumi.log.info("  Harbor Registry:")
pulumi.log.info("    Username: admin")
pulumi.log.info("    Password: Harbor12345")
pulumi.log.info("")
pulumi.log.info("NodePort access (for direct pod access):")
pulumi.log.info("  - Lagoon API: http://localhost:30030/graphql")
pulumi.log.info("  - Keycloak: http://localhost:30370/auth")
pulumi.log.info("  - Lagoon UI: http://localhost:31311")
pulumi.log.info("")

pulumi.log.info("=" * 80)
